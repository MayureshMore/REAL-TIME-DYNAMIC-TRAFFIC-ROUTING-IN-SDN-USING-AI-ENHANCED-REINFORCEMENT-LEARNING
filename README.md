# ğŸš¦ Real-Time Dynamic Traffic Routing in SDN using AI-Enhanced Reinforcement Learning

<p align="center">
  <img src="PROTOTYPE.JPG" alt="Prototype running on Raspberry Pi" width="50%">
</p>

---

## ğŸ§  Overview

This project implements a **real-time adaptive SDN controller** using **Reinforcement Learning (RL)** to dynamically select optimal network paths.  
It is built on **Ryu (OpenFlow 1.3)**, with **Mininet** as the network emulator, and integrates three RL agents:

- ğŸ¯ **Bandit Agent (Îµ-Greedy)** â€” baseline exploration/exploitation  
- ğŸ§  **LinUCB Agent** â€” contextual bandit with path-level features  
- ğŸ§¬ **DQN Agent** â€” deep neural network for state-based decision-making  

---

## âš™ï¸ System Architecture

**Closed-Loop Design:**

```
Mininet Topology (Two-Path / Fat-Tree)
        â†“
Ryu SDN Controller
  â†³ REST Telemetry (ports, flows)
  â†³ Path APIs (/paths, /actions/route)
        â†“
RL Agent (Bandit / LinUCB / DQN)
        â†“
Adaptive Path Reconfiguration
```

---

## ğŸ“‚ Repository Structure

```
REAL-TIME-DYNAMIC-TRAFFIC-ROUTING-IN-SDN-USING-AI-ENHANCED-REINFORCEMENT-LEARNING/
â”œâ”€â”€ controller-apps/
â”‚   â””â”€â”€ sdn_router_rest.py        # Unified controller + REST + stats
â”œâ”€â”€ rl-agent/
â”‚   â”œâ”€â”€ bandit_agent.py           # Îµ-greedy multi-armed bandit
â”‚   â”œâ”€â”€ linucb_agent.py           # contextual bandit (ridge regularized)
â”‚   â””â”€â”€ dqn_agent.py              # deep Q-network agent (PyTorch)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ topos/two_path.py         # dynamic topology generator
â”‚   â”œâ”€â”€ metrics/log_stats.py      # metrics logger
â”‚   â”œâ”€â”€ metrics/plot_results.py   # plotting utility
â”‚   â”œâ”€â”€ experiments/              # automated run scripts
â”‚   â””â”€â”€ run_ryu.sh                # controller launcher
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ baseline/                 # CSV logs + plots
â”‚   â”œâ”€â”€ api.md                    # REST documentation
â”‚   â””â”€â”€ openapi.yaml              # OpenAPI schema
â”œâ”€â”€ Makefile                      # automation entrypoint
â”œâ”€â”€ requirements.vm.txt           # minimal dependencies
â””â”€â”€ README.md                     # this file
```

---

## ğŸ§° Quick Start

### 1ï¸âƒ£ Install Dependencies
```bash
make setup
```

### 2ï¸âƒ£ Start the SDN Controller
```bash
make run-controller
```
Then check the health endpoint:
```bash
curl http://127.0.0.1:8080/api/v1/health
```

---

## ğŸ§ª Running Experiments

### Baseline (Shortest Path)
```bash
make run-baseline
```

### Bandit RL Agent (Îµ-Greedy)
```bash
make run-bandit
```

### LinUCB Contextual Bandit
```bash
make run-linucb
```

### Deep Q-Network Agent (PyTorch)
```bash
make run-dqn
```

### Generate Comparative Plots
```bash
make plot
```

Or for all experiments:
```bash
make compare
```

---

## ğŸ“ˆ Example Output

After each experiment, CSV logs are stored under:
```
docs/baseline/
 â”œâ”€â”€ ports_baseline_<timestamp>.csv
 â”œâ”€â”€ ports_rl_<timestamp>.csv
 â”œâ”€â”€ ports_dqn_<timestamp>.csv
 â””â”€â”€ plots/
     â”œâ”€â”€ throughput.png
     â”œâ”€â”€ drops.png
     â””â”€â”€ errors.png
```

Sample Plot (generated automatically):

<p align="center">
  <img src="docs/baseline/plots/throughput.png" width="80%">
</p>

---

## ğŸ§® Reward Function (Generalized)

For all agents, the reward is computed as:

```
R = (Throughput / MaxBW) â€“ Î±*(ErrorRate + DropRate)
```

where `Î±` is dynamically tuned (0.001â€“0.01).  
This balances high throughput with low packet loss.

---

## ğŸ§© RL Agent Comparison

| Agent | Algorithm | Context | Exploration | Advantage |
|--------|------------|----------|-------------|------------|
| **Bandit** | Îµ-Greedy | None | Fixed Îµ | Simple baseline |
| **LinUCB** | Contextual Bandit | Per-path metrics | UCB confidence | Stable performance |
| **DQN** | Deep Q-Network | Multi-feature vector | Îµ-decay | Learns nonlinear state-action mapping |

---

## ğŸŒ Topology Example

Run the two-path topology with custom parameters:

```bash
sudo python3 scripts/topos/two_path.py   --delay_a 5ms --loss_a 0.1   --delay_b1 15ms --delay_b2 15ms   --loss_b1 0.5 --loss_b2 0.5   --bw 20 --no_cli
```

---

## ğŸ“… Experiment Workflow

1. Start Controller â†’ `make run-controller`  
2. Launch Topology â†’ `make topo`  
3. Run RL Agent â†’ `make run-dqn` or `make run-linucb`  
4. Collect CSV Logs â†’ `docs/baseline/*.csv`  
5. Plot & Compare â†’ `make compare`

---

## ğŸ§ª Publication Readiness

This repository provides:
- **Reproducible experimental setup** with automated scripts  
- **Quantitative performance comparison** (baseline vs RL vs DQN)  
- **Deep RL component (DQN)** for novelty  
- **OpenAPI-documented REST interface** for integration  

Potential publication targets:
- IEEE ICNP / CNSM Demo Track  
- Elsevier *Computer Networks*  
- ACM NetAI or CoNEXT Student Workshop  

---

## ğŸ‘¨â€ğŸ’» Contributors

| Name | Role | Key Contributions |
|------|------|-------------------|
| **Mayuresh Sanjay More** | RL Agent & Experimentation Lead | Bandit, LinUCB, DQN, reward tuning |
| **Zeel Pankaj Patel** | Infrastructure & Automation | Mininet topologies, scripts, Ryu integration |
| **Omkar Gajendra Sutar** | Controller & REST API | Ryu app design, telemetry endpoints |

---

## âš¡ Future Enhancements

- [ ] Extend DQN to Double-DQN or Dueling DQN  
- [ ] Multi-agent cooperation for multiple flows  
- [ ] Topology generalization (Fat-Tree, Mesh)  
- [ ] Real-world Raspberry Pi deployment (edge-lab)  
- [ ] Publish results + graphs in academic demo paper  

---

